---
title: "Random forrest classifier -  Concept training"
author: "Frederik Beck"
date: "2025-08-21"

---


## Training of Random forest model on synthesized relative abundance data and meta data indicating either absence of presence of Disease in each individual
```{r}
# Load Packages

library(tidyverse)
library(caret)
library(randomForest)
library(pROC)

## 1. Load data
# Feature table (taxa as columns), metadata (with "Label")

X <- read.csv("microbiome_rel_abundance_1500x150.csv", row.names = 1, check.names = FALSE)
meta <- read.csv("microbiome_metadata_1500x150.csv", row.names = 1)


# Matching of samples

common <- intersect(rownames(X), rownames(meta))
X <- X[common, ]
meta <- meta[common, ]
y <- factor(meta$Label)

## 2. Train/Test split of data

set.seed(123)
idx <- createDataPartition(y, p = 0.7, list = FALSE)
Xtr <- X[idx, ]; Xte <- X[-idx, ]
ytr <- y[idx]; yte <- y[-idx]

## 3. Cross-validation setup
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,        
                     repeats = 2,     
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

## 4. Train Random Forest
rf <- train(x = Xtr, y = ytr,
            method = "rf",
            metric = "ROC",             
            trControl = ctrl,
            tuneGrid = data.frame(mtry = floor(sqrt(ncol(Xtr))) + (-2:2)),
            ntree = 1000,
            importance = TRUE)

print(rf)
# ROC vs mtry
plot(rf)   

## 5. Evaluate on test set

pred_class <- predict(rf, Xte)
pred_prob  <- predict(rf, Xte, type = "prob")

confusionMatrix(pred_class, yte)

# ROC & AUC
yte2 <- relevel(yte, ref = "Disease")   
roc_obj <- roc(yte2, pred_prob$Disease, levels = rev(levels(yte2)))
plot(roc_obj, main = "ROC curve (Random Forest)")
auc(roc_obj)

## 6. Feature importance

rf_raw <- rf$finalModel
imp <- importance(rf_raw, type = 1)  
imp_df <- data.frame(Taxon = rownames(imp), MeanDecreaseAccuracy = imp[,1]) %>%
  arrange(desc(MeanDecreaseAccuracy))

head(imp_df, 150)

ggplot(imp_df[1:150,], aes(x = reorder(Taxon, MeanDecreaseAccuracy),
                          y = MeanDecreaseAccuracy)) +
  geom_col() +
  coord_flip() +
  labs(title = "Top important taxa (Random Forest)",
       x = "Taxon", y = "Mean Decrease Accuracy")

#SAVE MODEL

saveRDS(rf, "rf_model.rds")

# Save the exact feature order the model was trained on
train_features <- colnames(Xtr)        
saveRDS(train_features, "rf_features.rds")

# Save any preprocessing choices
prep_info <- list(transform = "relative_abundance",         center_scale = FALSE,
                  notes = "Taxa columns only; samples in rows")
saveRDS(prep_info, "rf_prep_info.rds")


```



## Deployment of trained RF model
## =========================

## Inputs:
##   Trained model: rf_model.rds      (caret::train object)
##   Feature list: rf_features.rds    (character vector of column names used in training)
##   New data:     Lifelike_relative_abundance__preview_.csv  (samples x taxa, rownames = sample IDs)
##
## Output: predictions_on_lifelike_preview.csv  (class + probabilities per sample)
##
## Notes:
##   - Assumes training used relative abundances with no additional transforms.
##   - If you did log1p/CLR/etc. during training, apply the SAME transform here before prediction.

```{r}


suppressPackageStartupMessages({
  library(tidyverse)
})

# File paths (edit if needed) 
model_path    <- "rf_model.rds"
features_path <- "rf_features.rds"
newdata_path  <- "Lifelike_relative_abundance__preview_.csv"
out_path      <- "predictions_on_lifelike_preview.csv"

# Load artifacts
if (!file.exists(model_path)) stop("Model file not found: ", model_path)
if (!file.exists(features_path)) stop("Feature file not found: ", features_path)

rf <- readRDS(model_path)            
train_features <- readRDS(features_path)  

# Load new data

X_new <- read.csv(newdata_path, row.names = 1, check.names = FALSE)
if (!is.data.frame(X_new) || nrow(X_new) == 0) {
  stop("Loaded new data is empty or malformed: ", newdata_path)
}

# Coerce all columns to numeric (in case any were read as character)
num_cols <- suppressWarnings(
  sapply(X_new, function(col) {
    if (is.numeric(col)) return(TRUE)
    suppressWarnings(!any(is.na(as.numeric(col))))
  })
)
if (!all(num_cols)) {
  # Try to coerce; if still non-numeric, fail those columns explicitly
  X_new[] <- lapply(X_new, function(col) suppressWarnings(as.numeric(col)))
  if (anyNA(X_new)) {
    warning("Some values became NA during numeric coercion. Replacing NA with 0.")
    X_new[is.na(X_new)] <- 0
  }
}

# Apply SAME preprocessing as training 

# Align columns to training feature set
align_to_training <- function(X, train_feats) {
  missing <- setdiff(train_feats, colnames(X))
  if (length(missing) > 0) {
    X[missing] <- 0
  }

  extra <- setdiff(colnames(X), train_feats)
  if (length(extra) > 0) {
    X <- X[, !(colnames(X) %in% extra), drop = FALSE]
  }
  X <- X[, train_feats, drop = FALSE]
  return(X)
}

X_new_aligned <- align_to_training(X_new, train_features)

# Quick sanity checks
stopifnot(ncol(X_new_aligned) == length(train_features))
stopifnot(identical(colnames(X_new_aligned), train_features))

# Predict
# Class labels:
pred_class <- predict(rf, newdata = X_new_aligned)

# Class probabilities:
prob_df <- tryCatch(
  {
    as.data.frame(predict(rf, newdata = X_new_aligned, type = "prob"))
  },
  error = function(e) {
    warning("Probability prediction failed: ", conditionMessage(e))
    NULL
  }
)

#  Save results
out <- data.frame(
  SampleID = rownames(X_new_aligned),
  PredictedClass = as.character(pred_class),
  stringsAsFactors = FALSE
)
if (!is.null(prob_df)) {
  prob_df <- prob_df %>% as_tibble()
  names(prob_df) <- paste0("Prob_", names(prob_df))
  out <- bind_cols(out, prob_df)
}

write.csv(out, out_path, row.names = FALSE)
message("Wrote predictions to: ", normalizePath(out_path))

# quick peek

head(out)

out_long <- out %>%
  pivot_longer(cols = starts_with("Prob_"),
               names_to = "Class",
               values_to = "Probability")

# Heatmap
ggplot(out_long, aes(x = Class, y =SampleID , fill = Probability)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "grey", high = "blue") +
  labs(title = "Prediction probabilities heatmap",
       x = "Predicted class probability",
       y = "Sample ID") +
  theme_minimal() +
   theme(
    axis.text.y = element_text(size = 6),     
    axis.ticks.y = element_line(),             
    panel.grid = element_blank()
  )



```

