---
title: "Random forrest classifier -  Concept training"
author: "Frederik Beck"
date: "2025-08-21"
output: pdf_document
---


## Training of Random forest model on synthesized relative abundance data and meta data indicating either absence of presence of Disease in each individual
```{r}
# Load Packages

library(tidyverse)
library(caret)
library(randomForest)
library(pROC)

## 1. Load data
# Feature table (taxa as columns), metadata (with "Label")

X <- read.csv("microbiome_rel_abundance_1500x150.csv", row.names = 1, check.names = FALSE)
meta <- read.csv("microbiome_metadata_1500x150.csv", row.names = 1)


# Matching of samples

common <- intersect(rownames(X), rownames(meta))
X <- X[common, ]
meta <- meta[common, ]
y <- factor(meta$Label)

## 2. Train/Test split of data

set.seed(123)
idx <- createDataPartition(y, p = 0.7, list = FALSE)
Xtr <- X[idx, ]; Xte <- X[-idx, ]
ytr <- y[idx]; yte <- y[-idx]

## 3. Cross-validation setup
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,        
                     repeats = 2,     
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

## 4. Train Random Forest
rf <- train(x = Xtr, y = ytr,
            method = "rf",
            metric = "ROC",             
            trControl = ctrl,
            tuneGrid = data.frame(mtry = floor(sqrt(ncol(Xtr))) + (-2:2)),
            ntree = 1000,
            importance = TRUE)

print(rf)
# ROC vs mtry
plot(rf)





## 5. Evaluate on test set

pred_class <- predict(rf, Xte)
pred_prob  <- predict(rf, Xte, type = "prob")

confusionMatrix(pred_class, yte)

# ROC & AUC
yte2 <- relevel(yte, ref = "Disease")   
roc_obj <- roc(yte2, pred_prob$Disease, levels = rev(levels(yte2)))
plot(roc_obj, main = "ROC curve (Random Forest)")
auc(roc_obj)

## 6. Feature importance

rf_raw <- rf$finalModel
imp <- importance(rf_raw, type = 1)  
imp_df <- data.frame(Taxon = rownames(imp), MeanDecreaseAccuracy = imp[,1]) %>%
  arrange(desc(MeanDecreaseAccuracy))

head(imp_df, 150)

TopS <- ggplot(imp_df[1:30,], aes(x = reorder(Taxon, MeanDecreaseAccuracy),
                          y = MeanDecreaseAccuracy)) +
  geom_col() +
  coord_flip() +
  labs(title = "Top important taxa (Random Forest)",
       x = "Taxon", y = "Mean Decrease Accuracy")

print(TopS)

ggsave("Top30importanttaxa.png", plot = TopS, width = 16, height = 12, dpi = 300)


#SAVE MODEL

saveRDS(rf, "rf_model.rds")

# Save the exact feature order the model was trained on
train_features <- colnames(Xtr)        
saveRDS(train_features, "rf_features.rds")

# Save any preprocessing choices
prep_info <- list(transform = "relative_abundance",         center_scale = FALSE,
                  notes = "Taxa columns only; samples in rows")
saveRDS(prep_info, "rf_prep_info.rds")


```



## Deployment of trained RF model
## =========================

## Inputs:
##   Trained model: rf_model.rds      (caret::train object)
##   Feature list: rf_features.rds    (character vector of column names used in training)
##   New data:     Lifelike_relative_abundance__preview_.csv  (samples x taxa, rownames = sample IDs)
##
## Output: predictions_on_lifelike_preview.csv  (class + probabilities per sample)
##
## Notes:
##   - Assumes training used relative abundances with no additional transforms.
##   - If you did log1p/CLR/etc. during training, apply the SAME transform here before prediction.


```{r}
suppressPackageStartupMessages({
  library(tidyverse)
})


# Paths (edit if needed)

model_path    <- "rf_model.rds"
features_path <- "rf_features.rds"
newdata_path  <- "Lifelike_relative_abundance__preview_2.csv"
out_path      <- "predictions_on_lifelike_preview.csv"


# Load model artifacts

stopifnot(file.exists(model_path), file.exists(features_path))
rf <- readRDS(model_path)                 # caret::train object
train_features <- readRDS(features_path)  # character vector of feature names


# Load new data (keep Diet)
# Expect: first column = SampleID (best practice). If not, adjust row.names=...

df_raw <- read.csv(newdata_path, row.names = 2, check.names = FALSE)

# Try to find a diet column (case-insensitive)
diet_col <- intersect(colnames(df_raw), c("Diet","diet"))
meta_new <- tibble(SampleID = rownames(df_raw),
                   Diet = if (length(diet_col) == 1) df_raw[[diet_col]] else "Unknown")

# Keep only feature columns (drop Diet if present)
feature_df <- df_raw
if (length(diet_col) == 1) feature_df <- feature_df[, setdiff(colnames(feature_df), diet_col), drop = FALSE]

# Coerce features to numeric (without touching Diet)
feature_df[] <- lapply(feature_df, function(col) {
  if (is.numeric(col)) return(col)
  suppressWarnings(as.numeric(col))
})
# Replace any NA produced by coercion with 0
feature_df[is.na(feature_df)] <- 0


# Align to training features

align_to_training <- function(X, train_feats) {
  miss <- setdiff(train_feats, colnames(X))
  if (length(miss) > 0) X[miss] <- 0
  extra <- setdiff(colnames(X), train_feats)
  if (length(extra) > 0) X <- X[, !(colnames(X) %in% extra), drop = FALSE]
  X[, train_feats, drop = FALSE]
}

X_new_aligned <- align_to_training(feature_df, train_features)
stopifnot(identical(colnames(X_new_aligned), train_features))


# Predict

pred_class <- predict(rf, newdata = X_new_aligned)
prob_df <- tryCatch(
  as.data.frame(predict(rf, newdata = X_new_aligned, type = "prob")),
  error = function(e) { warning("Probability prediction failed: ", conditionMessage(e)); NULL }
)


# Save results (merge Diet)

out <- tibble(
  SampleID = rownames(X_new_aligned),
  PredictedClass = as.character(pred_class)
) %>%
  left_join(meta_new, by = "SampleID")

if (!is.null(prob_df)) {
  names(prob_df) <- paste0("Prob_", names(prob_df))
  out <- bind_cols(out, as_tibble(prob_df))
}

write.csv(out, out_path, row.names = FALSE)
message("Wrote predictions to: ", normalizePath(out_path))

# Heatmap faceted by Diet

out_long <- out %>%
  pivot_longer(cols = starts_with("Prob_"),
               names_to = "Class",
               values_to = "Probability")

Heat <- ggplot(out_long, aes(x = Class, y = SampleID, fill = Probability)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "grey", high = "#297A3F") +  
  labs(title = "Prediction probabilities by Diet",
       x = "Predicted class",
       y = "Sample ID") +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 6),
    axis.ticks.y = element_line(),
    panel.grid = element_blank(),
    strip.text = element_text(face = "bold")
  ) +
  facet_wrap(~ Diet, scales = "free_y")

ggsave("Probability_heatmap_by_Diet.jpg", plot = Heat, width = 10, height = 7, dpi = 300)

print(Heat)

```

